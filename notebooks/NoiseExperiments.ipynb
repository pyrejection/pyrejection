{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from pyrejection.datasets import EXP_NOISY_DATASETS, DATASETS\n",
    "from pyrejection.experiments import run_experiments\n",
    "from pyrejection.evaluation import (save_tradeoff_reports, get_experiment_stats,\n",
    "                                    ct_vs_nl_statistical_test, noise_feature_influence_plots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'accuracy'\n",
    "classifier = 'logreg'\n",
    "noisy_datasets = list(EXP_NOISY_DATASETS.keys())\n",
    "radial_datasets = [dataset for dataset in DATASETS.keys() if dataset.startswith('radial-synthetic-exp-noise')]\n",
    "datasets = noisy_datasets + radial_datasets\n",
    "sample_random_states = range(100)\n",
    "test_size = 0.3\n",
    "cache_dir = 'results_cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_experiments(random_states, discard_results=False, drop_test_preds=True):\n",
    "    return run_experiments(\n",
    "        [metric],\n",
    "        [classifier],\n",
    "        datasets,\n",
    "        random_states=random_states,\n",
    "        test_size=test_size,\n",
    "        worker_count=4,\n",
    "        cache_dir=cache_dir,\n",
    "        drop_test_preds=drop_test_preds,\n",
    "        discard_results=discard_results,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run All Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_experiments(sample_random_states[:1], discard_results=True, drop_test_preds=False)\n",
    "do_experiments(sample_random_states, discard_results=True)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Experiment Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load first set of results for reports.\n",
    "first_results = do_experiments(sample_random_states[:1])\n",
    "reports_base_dir = os.path.join('tradeoff_reports')\n",
    "save_tradeoff_reports(first_results, reports_base_dir)\n",
    "del first_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampled Statistics Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_stats = {dataset: {'ct': [], 'nl': []} for dataset in datasets}\n",
    "# Load summary statistics from each sample of experiments.\n",
    "for random_state in sample_random_states:\n",
    "    exp_results = do_experiments([random_state])\n",
    "    for exp_result in exp_results:\n",
    "        dataset = exp_result['config']['dataset']\n",
    "        # Add statistics from this experiment to the sample_evaluation.\n",
    "        for rej_method, stats in get_experiment_stats(exp_result).items():\n",
    "            sample_stats[dataset][rej_method].append(stats)\n",
    "    # Delete the loaded results to save memory.\n",
    "    del exp_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform statistical tests\n",
    "stat_names = sample_stats[datasets[0]]['ct'][0].keys()\n",
    "statistical_tests = {\n",
    "    stat: {\n",
    "        dataset: {\n",
    "            **ct_vs_nl_statistical_test(test_size,\n",
    "                                        [stats[stat] for stats in method_stats['ct']],\n",
    "                                        [stats[stat] for stats in method_stats['nl']]),\n",
    "        }\n",
    "        for dataset, method_stats in sample_stats.items()\n",
    "    }\n",
    "    for stat in stat_names\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "rows = []\n",
    "for dataset in datasets:\n",
    "    row = {'dataset': dataset}\n",
    "    for stat in stat_names:\n",
    "        res = statistical_tests[stat][dataset]\n",
    "        row[f'{stat}-ct-mean'] = res['ct-mean']\n",
    "        row[f'{stat}-nl-mean'] = res['nl-mean']\n",
    "        row[f'{stat}-max-stddev'] = max(res[\"ct-std\"], res[\"nl-std\"])\n",
    "        row[f'{stat}-nl-stddev-less'] = res[\"nl-std\"] < row[f'{stat}-max-stddev']\n",
    "        row[f'{stat}-t-test-significant'] = res['ct-vs-nl-2t-p'] < alpha\n",
    "        row[f'{stat}-wilcox-significant'] = res['ct-vs-nl-2w-p'] < alpha\n",
    "    rows.append(row)\n",
    "summary_df = pd.DataFrame(rows)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatted Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def format_bold(text):\n",
    "    return r'\\textbf{' + text + '}'\n",
    "\n",
    "stat_formatting = {\n",
    "    'Capacity': {'formatter': '{:.3f}', 'bold_greater': True},\n",
    "    'E^u at 80% C': {'formatter': '{:.1%}', 'bold_greater': False},\n",
    "    'C at 50% of Original E^u': {'formatter': '{:.1%}', 'bold_greater': True},\n",
    "}\n",
    "\n",
    "latex_table_rows = []\n",
    "for dataset in datasets:\n",
    "    row = {'Dataset': dataset}\n",
    "    for stat in stat_names:\n",
    "        res = statistical_tests[stat][dataset]\n",
    "        num_formatter = stat_formatting[stat]['formatter']\n",
    "        row[f'{stat} - CT'] = num_formatter.format(res['ct-mean'])\n",
    "        row[f'{stat} - NL'] = num_formatter.format(res['nl-mean'])\n",
    "        row[f'{stat} - Ïƒ'] = num_formatter.format(max(res[\"ct-std\"], res[\"nl-std\"]))\n",
    "        if res['ct-vs-nl-2t-p'] < alpha:\n",
    "            if res['ct-mean'] > res['nl-mean']:\n",
    "                if stat_formatting[stat]['bold_greater']:\n",
    "                    row[f'{stat} - CT']  = format_bold(row[f'{stat} - CT'])\n",
    "                else:\n",
    "                    row[f'{stat} - NL']  = format_bold(row[f'{stat} - NL'])\n",
    "            elif res['ct-mean'] < res['nl-mean']:\n",
    "                if stat_formatting[stat]['bold_greater']:\n",
    "                    row[f'{stat} - NL']  = format_bold(row[f'{stat} - NL'])\n",
    "                else:\n",
    "                    row[f'{stat} - CT']  = format_bold(row[f'{stat} - CT'])\n",
    "            else:\n",
    "                raise Exception('Difference should not be significant if mean is equal.')\n",
    "    latex_table_rows.append(row)\n",
    "latex = pd.DataFrame(latex_table_rows).to_latex(index=False)\n",
    "latex = latex.replace('\\\\textbackslash ', '\\\\')\n",
    "latex = latex.replace('\\\\{', '{')\n",
    "latex = latex.replace('\\\\}', '}')\n",
    "latex = latex.replace('  ', ' ')\n",
    "latex = re.sub(' +', ' ', latex)\n",
    "print(latex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise Feature Influence Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_results = do_experiments(sample_random_states[:1])\n",
    "dataset_options = {exp_result['config']['dataset']: exp_result\n",
    "                   for i, exp_result in enumerate(first_results)}\n",
    "widgets.interact(noise_feature_influence_plots,\n",
    "                 exp_result=widgets.Dropdown(description='Dataset: ', options=dataset_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
